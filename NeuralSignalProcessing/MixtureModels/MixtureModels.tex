% !TEX TS-program = xelatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}

\usepackage{parskip}
\usepackage{framed}
\usepackage{amsmath,amsthm,amssymb}

\newtheorem*{conjecture}{Conjecture}

\usepackage{titlesec}

\usepackage[svgnames]{xcolor}

%\usepackage{enumerate}
\usepackage{enumitem}
\newcounter{descriptcount}


%\usepackage{mathpazo}
\usepackage{mathtools}
\usepackage{unicode-math}
\usepackage{empheq}
\usepackage[most]{tcolorbox}
\usepackage{cancel}

\usepackage{caption}

\usepackage{xunicode} %handle unicode
\usepackage{xltxtra} %XeTeX extras
\usepackage{fontspec} %use OTF/TTF fonts

%\newcommand{\lmr}{\fontfamily{lmr}\selectfont} % Latin Modern Roman


%\setmainfont{Myriad Pro} %use this font
%\setmathfont{Adobe Garamond Pro} %use this font for math

\titleformat{\section}
  {\large\bf}{\thesection}{0.25em}{}[\titlerule]
\titlespacing{\section}
  {0pt}{*1.5}{0.25em}

\titleformat{\subsection}
  {\normalfont\bf}{\thesubsection}{0.25em}{}
\titlespacing{\subsection}
  {0pt}{*1}{0.125em}

\renewcommand\thesection{\Alph{section}.}
\renewcommand\thesubsection{\thesection\arabic{subsection}}
\renewcommand\thesubsubsection{(\roman{subsubsection}).}

\renewcommand{\labelitemi}{\textemdash}

\DeclareMathOperator{\dif}{d\!}
%\DeclareMathOperator{\Pr}{P}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\F}{\mathfrak{F}}
\DeclareMathOperator{\Poisson}{Poisson}
\DeclareMathOperator{\Bernoulli}{Bernoulli}
\DeclareMathOperator{\Binomial}{Binomial}
\DeclareMathOperator{\Order}{O}
\DeclareMathOperator{\Uniform}{Uniform}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\KLsym}{KL}
\DeclarePairedDelimiterX{\klx}[2]{(}{)}{%
  #1\,\delimsize\|\,#2%
}
\newcommand{\KL}{\KLsym\klx}


\newcommand{\logdet}[1]{\log \left| {#1} \right| }

\newcommand{\xb}{\mathbf{x}}
\newcommand{\ub}{\symbf{\mu}}
\newcommand{\Sb}{\symbf{\Sigma}}

\newcommand*{\tran}{^{\mkern-1.5mu\mathsf{T}}}
\newcommand*{\invtran}{^{\mkern-1.5mu\mathsf{-T}}}


\newenvironment{propertybox}{%
   \def\FrameCommand{\colorbox{LightSteelBlue}}%
   \MakeFramed{\advance\hsize-\width \FrameRestore}}
 {\endMakeFramed}

\lhead{\textbf{ELEC548}}
\chead{Mixture Models and Expectation Maximization}
\cfoot{}
\rhead{}
\rfoot{\thepage}
\pagestyle{fancyplain}

%\setlength\parindent{0pt}

\allowdisplaybreaks


\begin{document}
\setmainfont{Myriad Pro} %use this font
%\setmathfont{Latin Modern Math}
%\setmathfont{TG Pagella Math}

\begin{center}
\large
\textbf{ELEC 548} Mixture Models and Expectation Maximization
\end{center}

\section{Gaussian Mixtures}
In general, a \textit{mixture model} can be defined for arbitrary probability
distributions. However as is almost always the case, the Gaussian distribution makes
everything work nicely. Mixture models are a key foundation for machine learning based
on probabilistic generative models.

We will use a \textit{Mixture of Guassians} as a probabilistic generative model
for clustering. A good reference for this topic is Chapter 9 of Bishop's
\textit{Pattern Recognition and Machine Learning}. The first step to generate
a data point is to start with the cluster identity.

\begin{flalign*}
  \text{Let} \; z \in \lbrace 1, \ldots, K\rbrace \;
  \text{be a discrete random variable.} &
\end{flalign*}

We can define the probability of drawing a point from an individual mixture component
(i.e., cluster) as

\begin{equation}
  \Pr(z = k) = \pi_k, \; \text{where} \; k = 1, \ldots, K.
  \label{eqn:gmmPrior}
\end{equation}

Note that for $\Pr(z)$ to be a valid probability distribution, $0 \leq \pi_k
\leq 1$ and $\sum_{k = 1}^{K} \pi_k = 1$.

\begin{propertybox}
\textbf{Aside:} We have begun our discussion of mixture models implicitly
assuming that the number of clusters, $K$, is specified. Non-parametric models,
for which $K$ would not be prespefied, have been an area of active development
in the last decade. A good introduction to this topic is Chapter 25 of Murphy
\textit{Machine Learning: A Probabilistic Perspective}.
\end{propertybox}

Once we have our \textit{prior probability}, $\Pr(z)$, we can define the
distribution of the data point from the selected mixture component

\begin{equation}
  \Pr(\xb \mid z = k) = \mathcal{N} (\xb \mid \ub_k, \Sb_k)
  \label{eqn:gmmConditional}
\end{equation}

Combining the prior probability of the cluster and the distribution of data
points within the cluster we get the marginal probability of an observed data
point:

\begin{equation}
\Pr(\xb) = \sum_{z} \Pr(\xb \mid z) \Pr(z)
         = \sum_{k=1}^K \mathcal{N} (\xb \mid \ub_k, \Sb_k) \pi_k.
\end{equation}

{\bf A peak ahead:} We will use the $\Pr(\xb)$ density with training data to do
\textbf{ maximum likelihood parameter estimation}. Then, with optimized
parameters we can use the \textit{a posteriori} density, $\Pr(z \mid \xb)$, to
assign data points to clusters either with a ``''hard decision'' (choosing the
class which maximizes the density) or with a ``soft decision'' (keeping track of
how likely the datapoint is to have come from each cluster).

\subsection{Maximum likelihood parameter estimation}
\begin{flalign*}
  \text{\bf Goal:} \quad \text{Fit} \; \ub_k, \Sb_k, \pi_k \; \text{to training data} \;
      \xb_1, \ldots, \xb_N. &
\end{flalign*}

We will use the notational shorthand $\lbrace \xb \rbrace$ for the training data
$\xb_1, \ldots \xb_N$, and the shorthand $\theta$ for all the parameters of the
model $\lbrace \ub_k, \Sb_k, \pi_k\rbrace_{k = 1, \ldots, K}$. Then, we can
write down the likelihood of our training data
\begin{align*}
	\Pr (\lbrace \xb \rbrace \mid \theta ) &= \prod_{n=1}^N \Pr(\xb_n) \\
  &= \prod_{n=1}^N \sum_{k=1}^K \mathcal{N} (\xb \mid \ub_k, \Sb_k) \pi_k \\
  \underbrace{\log \Pr(\lbrace \xb \rbrace \mid \theta)}_{
    \mathclap{\text{\normalsize call this $\mathcal{L}$}}}
    &= \sum_{n=1}^N \log\left[ \sum_{k=1}^K \mathcal{N} (\xb \mid \ub_k, \Sb_k) \pi_k \right].
\end{align*}

\subsubsection{Find $\ub_k$:}
\begin{equation}
  \frac{\partial\mathcal{L}}{\partial\ub_k} = \sum_{n=1}^N
    \frac{1}{\sum_{j=1}^K \mathcal{N}(\xb_n \mid \ub_j, \Sb_j) \cdot \pi_j}
    \cdot \pi_k \cdot \left(\frac{\partial}{\partial \ub_k}
                            \mathcal{N}(\xb_n \mid \ub_k, \Sb_k)\right)
  \label{eqn:dLmu}
\end{equation}

Looking at our handy table of matrix derivatives, we find that for a symmetric
$\mathbf{A}$, $\frac{\partial}{\partial \mathbf{x}} \left(\xb\tran\!
\mathbf{A} \xb \right) = 2 \mathbf{A}\xb$. Then, applying the chain rule, we
have
\begin{align*}
  \frac{\partial}{\partial \ub} \mathcal{N}(\xb \mid \ub, \Sb) &=
    \frac{\partial}{\partial \ub}
    \left( \frac{1}{(2 \pi)^\frac{D}{2}\left|\Sb\right|^\frac{1}{2}}
              e^{-\frac{1}{2} (\xb - \ub)\tran \Sb^{-1} (\xb-\ub)}\right) \\
  &= \mathcal{N}(\xb \mid \ub, \Sb) \cdot \Sb^{-1} (\xb - \ub).
\end{align*}

Plugging this into \eqref{eqn:dLmu}, we have
\begin{align*}
  \frac{\partial\mathcal{L}}{\partial\ub_k} &= \sum_{n=1}^N
    \underbrace{\frac{\mathcal{N}(\xb_n \mid \ub_k, \Sb_k) \cdot \pi_k}
    {\sum_{j=1}^K \mathcal{N}(\xb_n \mid \ub_j, \Sb_j) \cdot \pi_j}}_{
      \mathclap{\text{\normalsize call this $\gamma_{nk}$} } }
    \cdot \Sb^{-1} (\xb - \ub) \\
  &= \Sb_k^{-1} \sum_{n=1}^{N} \gamma_{nk} (\xb - \ub) \\
  &= 0
\end{align*}
Defining $N_k = \sum_{n=1}^N \gamma_{nk}$, we   the result
\begin{equation}
  \tcboxmath{\ub_k = \frac{1}{N_k} \sum_{n=1}^N \gamma_{nk} \xb_n}
  \label{eqn:mu}
\end{equation}

Notice that this result is very similar to the cluster update for K-means, with
$\gamma_{nk}$ replacing $r_{nk}$ as tge ``responsibility'' that cluster $k$
takes in explaining the observation $\xb_n$. Notice that
$0 \leq \gamma_{nk} \leq 1$ and $\sum_{k=1}^K \gamma_{nk} = 1$. Let's think
about this by looking at what $\gamma_{nk}$ is:

\begin{align*}
  \gamma_{nk} &= \Pr (z_n = k \mid \xb_n) \\
    &= \frac{\Pr(\xb_n \mid z_n = k) \Pr(z_n = k)}{\Pr(\xb_n)} \\
    &= \frac{\mathcal{N}(\xb_n \mid \ub_k, \Sb_k) \cdot \pi_k}
    {\sum_{j=1}^K \mathcal{N}(\xb_n \mid \ub_j, \Sb_j) \cdot \pi_j}
\end{align*}
\begin{center}
%\includegraphics[scale=0.5]{Figure1.jpg}
\end{center}

So, while $\pi_k$ is the \underline{prior} probability that $z_n = k$,
$\gamma_{nk}$ is the \underline{posterior} probability that $z_n = k$
after we have observed $\xb_n$.

Unlike in the case of K-means, where training data points are assigned to a
particular cluster and $N_k$ is the number of points in that cluster, for the
mixture model, we can think of $N_k$ as the ``effective number'' of points in
cluster $k$.

Thus, in equation \eqref{eqn:mu}, $\ub_k$ is a weighted mean of the training
data, where the weights are given by the responsibilities, $\gamma_{nk}$. A
small value of $\gamma_{nk}$ means that cluster $k$ bears little responsiblity
for data point $\xb_n$ and a large value means that it is very repsonsible for
$\xb_n$. \eqref{eqn:mu} is very similar to the cluster update in K-means,
exect that rather than hard assignments to each cluster -- $r_{nk} = 0 \;
\text{or} \; 1$ -- each data point has a soft assignment based on $\gamma_{nk}$.

\subsubsection{Find $\Sb_k$:}
Looking at our handy table of matrix derivatives, we find that for a symmetric
$\frac{\partial}{\partial \mathbf{X}} \Tr\left(\mathbf{X}^{-1} \mathbf{A} \right)
= -\mathbf{X}^{-1} \mathbf{A} \mathbf{X}^{-1}$. Then, applying product and
chain rules, we have
\begin{align*}
  \frac{\partial}{\partial \Sb} \mathcal{N}(\xb \mid \ub, \Sb) &=
    \frac{\partial}{\partial \Sb}
    \left( \frac{1}{(2 \pi)^\frac{D}{2}\left|\Sb\right|^\frac{1}{2}}
              e^{-\frac{1}{2} (\xb - \ub)\tran \Sb^{-1} (\xb-\ub)}\right) \\
  &= \frac{1}{(2 \pi)^\frac{D}{2}} \left (
      e^{-\frac{1}{2} (\xb - \ub)\tran \Sb^{-1} (\xb-\ub)}
      \frac{\partial}{\partial \Sb} \frac{1}{\left|\Sb\right|^\frac{1}{2}} +
      \frac{1}{\left|\Sb\right|^\frac{1}{2}} \frac{\partial}{\partial \Sb}
    e^{-\frac{1}{2} (\xb - \ub)\tran \Sb^{-1} (\xb-\ub)}
    \right) \\
  &= \frac{1}{(2 \pi)^\frac{D}{2}}
    e^{-\frac{1}{2} (\xb - \ub)\tran \Sb^{-1} (\xb-\ub)}
    \Bigg[
      \left(-\frac{1}{2}\frac{\left|\Sb\right|\Sb^{-1}}
                             {\left|\Sb\right|^\frac{3}{2}}
      \right) \\
      &\qquad\qquad\qquad+ \frac{1}{\left|\Sb\right|^\frac{1}{2}}
        \left(-\frac{1}{2} \frac{\partial}{\partial\Sb}
        \Tr\left(\Sb^{-1}(\xb-\ub)(\xb-\ub)\tran\right)\right)
    \Bigg] \\
    &= -\frac{1}{2} \mathcal{N}(\xb \mid \ub, \Sb) \left(
      \Sb^{-1} - \Sb^{-1} (\xb-\ub) (\xb-\ub) \tran \Sb^{-1} \right)
\end{align*}

Thus, we have
\begin{align*}
  \frac{\partial\mathcal{L}}{\partial\Sb_k} &= \sum_{n=1}^N
    \frac{1}{\sum_{j=1}^K \mathcal{N}(\xb_n \mid \ub_j, \Sb_j) \cdot \pi_j}
    \cdot \pi_k \cdot \left(\frac{\partial}{\partial \Sb_k}
                            \mathcal{N}(\xb_n \mid \ub_k, \Sb_k)\right) \\
  &= -\frac{1}{2} \sum_{n=1}^N \gamma_{nk} \left(
    \Sb^{-1} - \Sb^{-1} (\xb-\ub) (\xb-\ub) \tran \Sb^{-1}
  \right) \\
  &=0
\end{align*}
Rearranging,
\begin{align*}
  \Sb^{-1} \sum_{n=1}^N \gamma_{nk}  &=
   \sum_{n=1}^N \gamma_{nk} (\xb-\ub) (\xb-\ub) \tran \Sb^{-1} \\
  \Sb^{-1} N_k &= \Sb^{-1}
    \left(\sum_{n=1}^N\gamma_{nk} (\xb-\ub)(\xb-\ub)\tran\right) \Sb^{-1}
\end{align*}
Front and back multiplying by the covariance matrix, $\Sb$, we have our result
\begin{equation}
  \tcboxmath{\Sb_k = \frac{1}{N_k} \sum_{n=1}^N \gamma_{nk}
  (\xb_n - \ub) (\xb_n - \ub)\tran}.
  \label{eqn:sigma}
\end{equation}
As with the means, this is simply a weighted sample covariance of the training
data.

\subsubsection{Find $\pi_k$:}
Finding a solution for $\pi_k$, the prior probilities of mixture component $k$
is slightly more complicated, because of the constraint that the prior
probabilities must sum to 1. Rather than a set-the-derivative-equal-to-zero
maximization, this is a \textit{constrained} maximization. This can be done
with the Lagrange multiplier technique. Instead of maxizing $\mathcal{L}$, we
maximize

\begin{equation*}
  \mathcal{L}'  = \mathcal{L} + \lambda
    \left( \sum_{k=1}^K \pi_k\right),
\end{equation*}
where $\lambda$ is the Lagrange multiplier who's value we'll discover by
enforcing the constraint on the solution.
\begin{align*}
  \frac{\partial \mathcal{L}'}{\partial \pi_k}
      &= \frac{\partial \mathcal{L}}{\partial \pi_k} + \lambda \\
  &= \sum_{n=1}^N
    \frac{1}{\sum_{j=1}^K \mathcal{N}(\xb_n \mid \ub_j, \Sb_j) \cdot \pi_j}
    \cdot \mathcal{N}(\xb_n \mid \ub_k, \Sb_k) + \lambda \\
    &= \sum_{n=1}^N \frac{\gamma_{nk}}{\pi_k} + \lambda = 0
\end{align*}
Rearranging and multiplying both sides by $\pi_k$,
\begin{gather*}
    \sum_{n=1}^N \gamma_{nk} = - \lambda \cdot \pi_k \\
    \implies \pi_k = \frac{-N_k}{\lambda}
\end{gather*}
Now, we can enforce the constraint that $\sum_{k=1}^K \pi_k = 1$.
\begin{gather*}
    -\frac{\sum_{k=1}^K N_k}{\lambda} = 1 \\
    \implies \lambda = -N
\end{gather*}
Thus,
\begin{equation}
  \tcboxmath{\pi_k = \frac{N_k}{N}}.
  \label{eqn:pi}
\end{equation}
In other words, the prior probability of each mixture component is the effective
fraction of training data for which the component is given responsibility. If
we were doing hard assignment, this would just be the number of points assigned
to the component.

\textbf{Wait a second!} The maximum likelihood parameters given by
\eqref{eqn:mu}, \eqref{eqn:sigma}, and \eqref{eqn:pi} are not actually in
closed form! The resposibilities, $\gamma{nk}$, appear in the right hand sides
of the equations, but depend on the values of the parameters. This suggests an
iterative solution -- using initial guesses of the parameters to estimate the
responsibilities, then recalculating the parameters. This turns out to be an
instance of the \textbf{Expectation Maximization (EM) Algorithm}.

The EM algorithm is a powerful and general method for optimizing the parameters
for models with \textbf{latent variables}. A latent variable is a random
variable defined as part of generative model that represents something that is
\text{not actually observed}. In the clustering mixture model, the $\{z_n\}$ are
latent variables -- unlike the $\{\xb_n\}$, they are never directly observed.

\subsection{EM Algorithm for Gaussian Mixture Model}
\begin{framed}
\begin{description}[%
  before={\setcounter{descriptcount}{0}},%
  ,font=\bfseries\stepcounter{descriptcount}\thedescriptcount.~]
  \item[Initialize] parameters $\ub_k, \Sb_k, \pi_k \forall k \in 1,\ldots,K$.
              (Also need to decide on $K$!)
  \item[E-step:] Evaluate responsibilities given current parameter values
      \begin{equation*}
        \gamma_{nk} = \frac{\mathcal{N}(\xb_n \mid \ub_k, \Sb_k)}
                        {\sum_{j=1}^K \mathcal{N}(\xb_n \mid \ub_j, \Sb_j)}
      \end{equation*}
  \item[M-step:] Re-estimate parameters using the current values of the
    responsibilities
    \begin{align*}
      \ub_k^{\text{new}} &= \frac{1}{N_k} \sum_{n=1}^N \gamma_{nk} \xb_n \\
      \Sb_k^{\text{new}} &= \frac{1}{N_k} \sum_{n=1}^N \gamma_{nk}
            (\xb_n - \ub^{\text{new}}) (\xb_n - \ub^{\text{new}})\tran \\
      \pi_k^{\text{new}} &= \frac{N_k}{N}
    \end{align*}
    where $N_k = \sum_{n=1}^N \gamma_{nk}$.
  \item[Evaluate the log likelihood of training data:]
    \begin{equation*}
      \log \Pr(\{\xb_n\} \mid \theta) =
        \sum_{n=1}^N \log \left[ \sum_{k=1}^K
              \mathcal{N}(\xb_n \mid \ub_k, \Sb_k) \cdot \pi_k \right]
    \end{equation*}
    in order to see whether it has converged. (Alternatively, one can track the
    convergence of the parameters after step \textbf{3}.) If the convergence
    criteria are not satisfied go to step \textbf{2}.
\end{description}
\end{framed}

After each iteration of the EM algorithm the log likelihood of the training
data, $\log \Pr(\{\xb_n\} \mid \theta)$, increases (meaning that the parameter
estimates are improving).

\textbf{Mixtures of Exponential Family Distributions:} We have calculated the
M-step parameter updates for clustering using a mixture of Gaussians model. The
solution ends up being weighted versions of the typical maximum-likelihood
parameter estimates for $\ub_k$ and $\Sb_k$. Here's a conjecture
\begin{conjecture}
  For a mixture model defined using an distribution from the exponential family,
  the parameter updates in the M-step of the EM algorithm will take the form
  of weighted averages of the sample estimates of these parameters.
\end{conjecture}
Here's the outline of a proof.
\begin{proof}
If we consider a distribution in the very general exponential family, we can
define
\begin{equation}
  \Pr(\xb \mid z = k) = f(\xb \mid \symbf{\eta}_k) = h(\xb)
      \exp \left(\symbf{\eta}_k\tran T(\xb) - A(\symbf{\eta}_k)\right)
  \label{eqn:expfamily}
\end{equation}
where $\symbf{\eta}_k$ are the so-called ``natural parameters'' of the
distribution $f(\xb)$ for component $k$. The typical parameters that we think of
may not be the natural ones, but can be simply derived from them. The functions
$h()$, $T()$, and $A()$ are determined by the type of distribution. If we have
some data $\{\xb_n\}$, for a general exponential family distribution, the
maximum likelihood estimates for the natural parameters are the solution to the
equation
\begin{equation*}
  \frac{\partial}{\partial \symbf{\eta}} A(\symbf{\eta}) = \frac{\sum_{n=1}^N T(\xb_n)}{N}.
\end{equation*}
So now let's consider our mixture model.
\begin{equation}
  \frac{\partial\mathcal{L}}{\partial\symbf{\eta}_k} = \sum_{n=1}^N
    \frac{1}{\sum_{j=1}^K f(\xb \mid \symbf{\eta}_j)}
    \cdot \pi_k \cdot \left(\frac{\partial}{\partial\symbf{\eta}_k}
                            f(\xb \mid \symbf{\eta}_k)\right)
\end{equation}
From \eqref{eqn:expfamily}, we can see that
\begin{equation*}
  \frac{\partial}{\partial\symbf{\eta}} f(\xb \mid \symbf{\eta}) =
  f(\xb) \left(T(\xb) - \frac{\partial}{\partial \symbf{\eta}}
    A(\symbf{\eta}) \right)
\end{equation*}
Plugging this in, we have
\begin{gather}
  \frac{\partial\mathcal{L}}{\partial\symbf{\eta}_k} = \sum_{n=1}^N
    \gamma_{nk} \left(T(\xb) - \frac{\partial}{\partial \symbf{\eta}_k}
      A(\symbf{\eta}_k) \right) = 0 \\
  \implies \frac{\partial}{\partial \symbf{\eta}_k} A(\symbf{\eta}_k) =
    \frac{1}{N_k} \sum_{n=1}^N \gamma_{nk} T(\xb).
\end{gather}
Notice that this has the same form as the general maximum likelihood parameter
estimate, but weighted by the responsibilities $\gamma_{nk}$.
\end{proof}

\section{Relating EM for a Guassian Mixture Model to Classification}

During the training phase, the goal for both a Gaussian Mixture Model (GMM) and
a Gaussian Classification Model is to estimate the model parameters from the
training data.

\underline{Key Difference:} In Classification, the class labels are known,
whereas in a GMM (or any Clustering problem), the class labels are not known.
When we solved for the maximum likelihood parameter estimates in the
Classification model, we found closed-form solutions:

\begin{align}
  \pi_k &= \frac{N_k}{N}, \text{ where } N_k = \sum_{n \in C_k} 1 \\
  \ub_k &= \frac{1}{N_k} \sum_{n \in C_k} \xb_n \\
  \Sb_k &= \frac{1}{N_k} \sum_{n \in C_k} (\xb_n - \ub_k) (\xb_n - \ub_k)\tran
  \label{eqn:classification}
\end{align}

When we do Clustering using a GMM, we don't have labels -- we don't know what
class training datum $\xb_n$ is in (i.e., whether $n \in C_k$). So we have a
chicken-egg problem:

\begin{itemize}
  \item If we knew the labels for the training data, then we could calculate ML
  parameters using \eqref{eqn:classification}.
  \item Conversely, if we knew the model parameters, then we could calculate
  $\Pr(C_k \mid \xb_n)$ and assign $\xb_n$ to a class, using, for example,
  the maximum a posteriori rule (i.e., $\argmax \Pr(C_k\mid\xb_n)$).
\end{itemize}

Using the EM algorithm, we can ``bootstrap'' our way out of this problem:

\begin{framed}
\textbf{Summary of EM Algorithm for Clustering}
\begin{description}[%
  before={\setcounter{descriptcount}{0}},%
  ,leftmargin=1cm,labelindent=1cm%
  ,font=\bfseries\stepcounter{descriptcount}\thedescriptcount.~]
  \item[Initialize] model parameters
  \item[E-step:] Estimate the class labels given the current estimates of the
  model parameters.
  \item[M-step:] Estimate the model parameters given the current estimates of
  the class labels (using the distribution over classes rather than hard
  assignments).
  \item[Go to step 2:] until convergence.
\end{description}
\end{framed}

So to conclude the comparison, in Clustering with a GMM, the \underline{training
phase} uses the EM algorithm in which
\begin{itemize}
  \item The E-step reminds us of the \textit{test phase} in Classification.
  \item The M-step reminds us of the \textit{training phase} in Classification.
\end{itemize}
In Clustering with a GMM, the \underline{test phase} (cluster assignment for
test or validation data) will typically involve running a single E-step on the
test data (i.e., without iteration). \underline{Note:} As the number of clusters
is a hyper parameter for a GMM, this is exactly the sort of situation where we
might split our test data into two groups, one ``test'' for testing different
values of the number of clusters, and one ``validation'' for measuring the final
performance (e.g., for comparison to some other algorithm).

\section{The EM Algorithm in General}
\subsection{Motivation}
\textit{What is the motivation of the EM algorithm in general?} Imagine that you
have a model that you'd like to fit to your training data. If the model has
\textit{latent variables} (which are ubiquitous in the case of neural signal
processing), the EM algorithm provides a recipe for fitting the model. Because
of this generality, the EM algorithm is among the most important and widely-used
tools in machine learning. We will see it again at least two more times in this
course!

\subsection{Decomposition of the data likelihood}

\begin{framed}
\begin{flalign*}
  \text{Let } \; & \mathbf{X} \text{ denote all the observed variables (i.e.,
    training data observations).} & \\
  \quad & \mathbf{Z}  \text{ denote all the latent variables.}\\
  \quad  & \symbf{\theta}  \text{ denote all the model parameters.}
\end{flalign*}
\underline{Goal:} Maximize $\Pr(\mathbf{X} \mid \symbf{\theta})$ with respect
to $\symbf{\theta}$.
\end{framed}
\underline{Note:} Normally, when we write down a latent variable model for our
data, we describe two distributions,
$\Pr(\mathbf{Z}\mid\mathbf{X},\symbf{\theta})$ and $\Pr(\mathbf{Z})$. See, for
example how we started our Gaussian Mixture model for clustering with equations
\eqref{eqn:gmmPrior} and \eqref{eqn:gmmConditional}. EM uses this model
description to maximize the data likelihood, $\Pr(\mathbf{X} \mid
\symbf{\theta})$. In principle one could directly maximize the data likelihood
without reference to the latent variables, but this quickly becomes unwieldy.

Our analysis of the EM algorithm begins by considering an arbitrary
distribution over the latent variables, $q(\mathbf{Z})$.For any choice of
$q(z)$, we can decomponse the data likelihood,
$\Pr(\mathbf{X}\mid\symbf{\theta})$, into a sum of two terms:
\begin{align}
  \log\Pr(\mathbf{X}\mid\symbf{\theta}) = \mathcal{L}(q, \symbf{\theta}) +
      \KL{q}{p_{\mathbf{Z}\mid\mathbf{X}}} \label{eqn:datalikelihood} \\
  \intertext{where}
  \mathcal{L}(q, \symbf{\theta}) = \sum_{\mathbf{Z}} q(\mathbf{Z})
    \log \frac{\Pr(\mathbf{X}, \mathbf{Z} \mid \symbf{\theta})}{q(\mathbf{Z})} \\
  \KL{q}{p_{\mathbf{Z}\mid\mathbf{X}}} = - \sum_{\mathbf{Z}} q(\mathbf{Z})
    \log \frac{\Pr(\mathbf{Z} \mid \mathbf{X}, \symbf{\theta})}{q(\mathbf{Z})}
\end{align}

\begin{itemize}
\item $\mathcal{L}(q, \symbf{\theta})$ is a scalar that depends only on the
values of the parameters, $\symbf{\theta}$ and the choice of $q(\mathbf{Z})$,
because $\mathbf{X}$ are observations and the latent variables, $\mathbf{Z}$,
are integrated out.

\item $\KL{q}{p_{\mathbf{Z}\mid\mathbf{X}}}$ is the Kullback-
Leibler divergence between $q(\mathbf{Z})$ and $\Pr(\mathbf{Z} \mid
\mathbf{X}, \symbf{\theta})$.

\item We've written these terms as sums over the latent
variables -- this is shorthand for a sum or integral depending on whether the
latent variables are discrete (as in the GMM) or continuous.
\end{itemize}

\begin{framed}
  \underline{Aside: What is a KL divergence?} The KL divergence is a scalar measure
  of the distance between probability distributions. In general, it's defined
  as
  \begin{equation*}
    \KL{q}{p} = -\sum_{v} q(v) \log \frac{p(v)}{q(v)}
  \end{equation*}

  Let's do a simple example
  \begin{center}
    \includegraphics[scale=0.5]{KLexample.pdf}
  \end{center}
  \begin{align*}
    \KL{q}{p} &= -\left(q(1) \log \frac{p(1)}{q(1)} +
          q(2) \log \frac{p(2)}{q(2)} + q(3) \log \frac{p(3)}{q(3)} \right) \\
        &= -\left( \frac{1}{2} \log \frac{\frac{1}{3}}{\frac{1}{2}} +
          \frac{1}{4} \log \frac{\frac{1}{3}}{\frac{1}{4}} +
          \frac{1}{4} \log \frac{\frac{1}{3}}{\frac{1}{4}} \right)
        = -\left( \frac{1}{2} \log \frac{2}{3} +
            \frac{1}{2} \log \frac{4}{3} \right) \\
        &= -\left( \log \frac{2}{3} +
            \frac{1}{2} \log 2 \right)
        = -\left( \frac{3}{2} \log 2 - \log 3 \right) = -0.0589
  \end{align*}

  Facts about the KL divergence
  \vspace{-2\baselineskip}
  \begin{align*}
    \KL{p}{p} &= -\sum_{v} p(v) \; \cancelto{0}{\log \frac{p(v)}{p(v)}} = 0. \\
    \KL{q}{p} &\geq 0 \; \text{ for any $p$ and $q$}. \\
    \KL{q}{p} &= 0 \; \text{ if and only if $p = q$}.
  \end{align*}
\end{framed}

With the KL divergence defined, let's verify \eqref{eqn:datalikelihood}.
\begin{align*}
  \mathcal{L}(q, \symbf{\theta}) &= \sum_{\mathbf{Z}} q(\mathbf{Z})
    \log \frac{\Pr(\mathbf{X}, \mathbf{Z} \mid \symbf{\theta})}{q(\mathbf{Z})} \\
    &= \sum_{\mathbf{Z}} q(\mathbf{Z}) \log \frac{
      \Pr(\mathbf{Z} \mid \mathbf{X}, \symbf{\theta})
      \Pr(\mathbf{X}\mid \symbf{\theta})}{q(\mathbf{Z})} \\
    &= \sum_{\mathbf{Z}} q(\mathbf{Z}) \log
      \frac{\Pr(\mathbf{Z} \mid \mathbf{X}, \symbf{\theta})}{q(\mathbf{Z})} +
      \sum_{\mathbf{Z}} q(\mathbf{Z}) \log\Pr(\mathbf{X}\mid \symbf{\theta})\\
    &= -\KL{q}{p_{\mathbf{Z}\mid\mathbf{X}}} +
      \log\Pr(\mathbf{X}\mid \symbf{\theta})
      \cancelto{1}{\sum_{\mathbf{Z}} q(\mathbf{Z})} \\
  \implies &\log\Pr(\mathbf{X}\mid \symbf{\theta}) =
    \mathcal{L}(q, \symbf{\theta}) + \KL{q}{p_{\mathbf{Z}\mid\mathbf{X}}} _{//}
\end{align*}

\begin{itemize}
\item Notice that $\KL{q}{p_{\mathbf{Z}\mid\mathbf{X}}} \geq 0$ with equality only
when our arbitrary $q(\mathbf{Z}) = \Pr(\mathbf{Z} \mid \mathbf{X},
\symbf{\theta})$.

\item Thus, ${\log\Pr(\mathbf{X}\mid\symbf{\theta})} \geq
\mathcal{L}(q, \symbf{\theta})$, meaning that $\mathcal{L}(q, \symbf{\theta})$
is a lower bound for $\log\Pr(\mathbf{X}\mid \symbf{\theta})$.

\item The bound is
tight when $q(\mathbf{Z})$ is chosen to be $\Pr(\mathbf{Z} \mid \mathbf{X},
\symbf{\theta})$.
\end{itemize}

Here's the picture to have in mind (\emph{PRML, Ch. 9, Figure 11}):
\begin{center}
  \includegraphics[scale=1]{Figure9-11.pdf}
\end{center}


\subsection{The EM algorithm}
\begin{align*}
  \text{\underline{Goal:}} & \; \text{Maximize } \log \Pr(\mathbf{X} \mid \symbf{\theta})
    \text{ w.r.t. } \symbf{\theta}.  \\
  \text{\underline{Approach:}} & \; \text{Iteratively maximize the lower bound }
    \mathcal{L}(q, \symbf{\theta}) \text { w.r.t. } q() \text{ and }
    \symbf{\theta}. \\
  &\text{E-step: Maximize } \mathcal{L}(q, \symbf{\theta})
    \text{w.r.t. $q()$ while keeping $\symbf{\theta}$ fixed} \\
  &\text{M-step: Maximize } \mathcal{L}(q, \symbf{\theta})
    \text{w.r.t. $\symbf{\theta}$ while keeping $q()$ fixed}
\end{align*}

\subsubsection*{E-step}
With fixed $\symbf{\theta}$, $\mathcal{L}(q, \symbf{\theta})$ is maximized when
$\KL{q}{p_{\mathbf{Z}\mid\mathbf{X}}} = 0$, corresponding to $q(\mathbf{Z}) =
\Pr(\mathbf{Z} \mid \mathbf{X},\symbf{\theta})$.

\begin{center}
  \includegraphics[scale=1]{Figure9-12.pdf}
\end{center}


\subsubsection*{M-step}
\begin{equation*}
  \mathcal{L}(q, \symbf{\theta}) = \sum_{\mathbf{Z}} q(\mathbf{Z})
    \log \Pr(\mathbf{X}, \mathbf{Z} \mid \symbf{\theta}) -
    \overbrace{\sum_{\mathbf{Z}} q(\symbf{\mathbf{Z}})
        \log q(\mathbf{Z})}^
    {\mathclap{\text{\normalsize no $\symbf{\theta}$ dependence}}}
\end{equation*}
With fixed $q()$, maximizing $\mathcal{L}(q, \symbf{\theta})$ is equivalent to
maximizing
\begin{equation*}
  \sum_{\mathbf{Z}} q(\mathbf{Z}) \log \Pr(\mathbf{X}, \mathbf{X} \mid
      \symbf{\theta}) \equiv \E_q [\log \Pr(\mathbf{X}, \mathbf{X} \mid
          \symbf{\theta})].
\end{equation*}
We refer to $\E_q [\log \Pr(\mathbf{X}, \mathbf{X} \mid \symbf{\theta})]$ as the
``expected log joint distribution''.

The picture now is that we increase our total data likelihood by picking the
best parameters.
\begin{center}
  \includegraphics[scale=1]{Figure9-13.pdf}
\end{center}
\begin{itemize}
  \item Because $\mathcal{L}(q, \symbf{\theta})$ is a lower bound on
  $\log \Pr (\mathbf{X} \mid \symbf{\theta})$,
  $\log \Pr (\mathbf{X} \mid \symbf{\theta})$ must increase by at least as much
  as $\mathcal{L}(q, \symbf{\theta})$.

  \item Its ``as least as much'' because now there will be a nonzero
  $\KL{q}{p}$ because $p = \Pr(\mathbf{Z} \mid \mathbf{x}, \symbf{\theta})$
  changes if the parameters change.
\end{itemize}

Looking at the last two pictures, we see that
$\log \Pr (\mathbf{X}\mid\symbf{\theta})$ is guaranteed not to decrease each EM
iteration. Thus, the EM algorithm is guaranteed to converge to a local maximum.
Here is a schematic of what the E and M steps of the iteration might look like
as a function of a parameter:
\begin{center}
  \includegraphics[scale=1]{Figure9-14.pdf}
\end{center}
The first E-step (blue) makes the lower bound tight, and then the first M-step
finds the peak of this distribution. The next E-step (green) makes the new
bound tight, etc.

When we run the EM iteration, we need to check for convergence after each step
we can plot $\Pr(\mathbf{x} \mid \symbf{\theta})$ at each time step to yield
a ``learning curve'', \underline{which will be non-decreasing each timestep}.
\begin{center}
  \includegraphics[scale=0.5]{LearningCurve.png}
\end{center}




\end{document}
